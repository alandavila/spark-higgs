{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 11 Million Monte Carlo simulations of nuclear collsions. Signal collisions correspond to collisions where a Higgs boson was created. Background collisions correspond to collisions that have the same end product particles but where a Higgs boson was not created. Each collision has 28 attributes.\n",
    "Data set location: [https://archive.ics.uci.edu/ml/datasets/HIGGS]\n",
    "\n",
    "Relevant Paper: * Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014) *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('higgs-analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_location = os.path.join('resources','HIGGS_subsampled_20k.csv')\n",
    "df = spark.read.load(data_location,\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1399830, 600170)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training, test) = df.randomSplit([0.7, 0.3])\n",
    "training.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.describe(training.columns[1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Not required for GBT or Random forrest but done to make it easy to add more classifiers later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dense = training.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "training_dense = spark.createDataFrame(training_dense, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dense = test.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "test_dense = spark.createDataFrame(test_dense, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = standardScaler.fit(training_dense)\n",
    "scaled_training = scaler.transform(training_dense)\n",
    "scaled_training.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test = scaler.transform(test_dense)\n",
    "scaled_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create GBT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib import linalg as mllib_linalg\n",
    "from pyspark.ml import linalg as ml_linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_old(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return mllib_linalg.SparseVector(v.size, v.indices, v.values)\n",
    "    if isinstance(v, ml_linalg.DenseVector):\n",
    "        return mllib_linalg.DenseVector(v.values)\n",
    "    raise ValueError(\"Unsupported type {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_labelPoint_train = scaled_training.rdd.map(lambda row: LabeledPoint(row.label, as_old(row.features_scaled)))\n",
    "scaled_labelPoint_train.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [0.2746966481208801,-1.9840284585952759,0.7694543600082397,1.4012621641159058,1.495957612991333,0.9564709663391113,0.33269181847572327,-0.03209567442536354,0.0,0.7049337029457092,-0.8722332715988159,-1.0061124563217163,2.214872121810913,0.6468133926391602,-0.3530036211013794,-0.6303791999816895,0.0,1.0685573816299438,-0.23031991720199585,1.3876736164093018,3.101961374282837,0.8952987790107727,0.9776041507720947,0.9931463003158569,0.7896732687950134,0.6978268623352051,0.7527646422386169,0.7791340947151184]),\n",
       " LabeledPoint(0.0, [0.27487966418266296,-2.014221668243408,-0.428039014339447,1.4417753219604492,-1.4561175107955933,0.9727770686149597,-0.5723783373832703,-0.5166195034980774,2.1730761528015137,1.4753371477127075,-1.29866623878479,0.7378231287002563,2.214872121810913,0.896195650100708,-0.8327046632766724,1.7023881673812866,0.0,0.48657089471817017,-0.54929119348526,-0.026345083490014076,0.0,0.7342715859413147,0.8667398691177368,0.9870530962944031,1.1631782054901123,1.7355798482894897,1.047628402709961,0.831914484500885])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelPoint_train = training_dense.rdd.map(lambda row: LabeledPoint(row.label, as_old(row.features)))\n",
    "labelPoint_train.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "train_start = time.time()\n",
    "GBTmodel = GradientBoostedTrees.trainClassifier(labelPoint_train,\n",
    "                                             categoricalFeaturesInfo={}, numIterations=30)\n",
    "train_end = time.time()\n",
    "print(f'Time elapsed training model: {train_end - train_start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute test error\n",
    "predictions = GBTmodel.predict(test_dense.rdd.map(lambda x: x.features.values))\n",
    "labelsAndPredictions = test_dense.rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(test_dense.rdd.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification GBT model:')\n",
    "#print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "train_start = time.time()\n",
    "RFmodel = RandomForest.trainClassifier(labelPoint_train,\n",
    "                                     numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     numTrees=30, featureSubsetStrategy=\"auto\",\n",
    "                                     impurity='gini', maxDepth=4, maxBins=32)\n",
    "train_end = time.time()\n",
    "print(f'Time elapsed training model: {train_end - train_start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = RFmodel.predict(test_dense.rdd.map(lambda x: x.features.values))\n",
    "labelsAndPredictions = test_dense.rdd.map(lambda lp: lp.label).zip(predictions)\n",
    "\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(test_dense.rdd.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification RF model:')\n",
    "#print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20k simulations: (train/test split 0.7/0.3)\n",
    "\n",
    "    GBT error = 0.3072; GBT time = 40.44 secs\n",
    "\n",
    "    RF  error = 0.3271; RF  time = 14.75 secs\n",
    "  \n",
    "200k simulations: (train/test split 0.7/0.3)\n",
    "\n",
    "    GBT error = 0.3066; GBT time = 57.71 secs\n",
    "\n",
    "    RF  error = 0.3366; RF  time = 13.00 secs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
